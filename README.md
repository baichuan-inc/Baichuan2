<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->

<div align="center">
<h1>
  Baichuan2
</h1>
</div>

<p align="center">
ğŸ¤— <a href="https://huggingface.co/baichuan-inc/" target="_blank">Hugging Face</a> â€¢ ğŸ¤– <a href="https://modelscope.cn/organization/baichuan-inc" target="_blank">ModelScope</a> â€¢ ğŸ’¬ <a href="https://github.com/baichuan-inc/Baichuan2/blob/main/media/wechat.jpeg?raw=true" target="_blank">WeChat</a>
</p>

<div align="center">

[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/baichuan-inc/Baichuan2/blob/main/LICENSE)
<h4 align="center">
    <p>
        <b>ä¸­æ–‡</b> |
        <a href="https://github.com/baichuan-inc/Baichuan2/blob/main/README_EN.md">English</a>
    <p>
</h4>
</div>

# ç›®å½•

- [ä»‹ç»](#ä»‹ç»)
- [Benchmarkç»“æœ](#Benchmark-ç»“æœ)
- [æ¨¡å‹ç»†èŠ‚](#æ¨¡å‹ç»†èŠ‚)
- [æ¨ç†å’Œéƒ¨ç½²](#æ¨ç†å’Œéƒ¨ç½²)
- [åº”ç”¨æ¡ˆä¾‹](#åº”ç”¨æ¡ˆä¾‹)
- [å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ](#å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ)
- [å†å² checkpoint](#å†å²-checkpoint)
- [ç¤¾åŒºæ”¯æŒ](#ç¤¾åŒºæ”¯æŒ)
- [å£°æ˜](#å£°æ˜)
- [åè®®](#åè®®)

# ä»‹ç»

Baichuan2 æ˜¯ç”±ç™¾å·æ™ºèƒ½ç»§ Baichuan-7B å’Œ Baichuan-13B ä¹‹ååšå‡ºçš„æ›´æ–°ç‰ˆæœ¬ï¼Œé‡‡ç”¨ 2.6 ä¸‡äº¿  Tokens çš„é«˜è´¨é‡è¯­æ–™è®­ç»ƒï¼Œåœ¨æƒå¨çš„ä¸­æ–‡å’Œè‹±æ–‡ benchmark ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚æœ¬æ¬¡å‘å¸ƒåŒ…å«æœ‰ 7Bã€13B çš„ Base å’Œ Chat ç‰ˆæœ¬ï¼Œå¹¶æä¾›äº† Chat ç‰ˆæœ¬çš„ 4bits é‡åŒ–ï¼Œæ‰€æœ‰ç‰ˆæœ¬ä¸ä»…å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¼€å‘è€…ä¹Ÿä»…éœ€é‚®ä»¶ç”³è¯·å¹¶è·å¾—å®˜æ–¹å•†ç”¨è®¸å¯åï¼Œå³å¯ä»¥å…è´¹å•†ç”¨ã€‚å…·ä½“å‘å¸ƒç‰ˆæœ¬å’Œä¸‹è½½è§ä¸‹è¡¨ï¼š
|         | åŸºåº§æ¨¡å‹  | å¯¹é½æ¨¡å‹ | å¯¹é½æ¨¡å‹ 4bits é‡åŒ– |
|:-------:|:-------:|:-------:|:-----------------:|
| 7B      | [Baichuan2-7B-Base](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base) |[Baichuan2-7B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat) |[Baichuan2-7B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat-4bits) |
| 13B     | [Baichuan2-13B-Base](https://huggingface.co/baichuan-inc/Baichuan2-13B-Base) |[Baichuan2-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat) |[Baichuan2-13B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat-4bits) |


# Benchmark ç»“æœ

æˆ‘ä»¬åœ¨é€šç”¨ã€æ³•å¾‹ã€åŒ»ç–—ã€æ•°å­¦å’Œä»£ç äº”ä¸ªé¢†åŸŸçš„ä¸­è‹±æ–‡æƒå¨æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ã€‚

>*è¡¨ç¤ºæ¨¡å‹ç»“æœä»å…¶å®˜æ–¹è·å–ã€‚

## é€šç”¨é¢†åŸŸ
åœ¨é€šç”¨é¢†åŸŸæˆ‘ä»¬åœ¨ [C-Eval](https://cevalbenchmark.com/index.html#home)ï¼Œ[MMLU](https://arxiv.org/abs/2009.03300) å’Œ [CMMLU](https://github.com/haonan-li/CMMLU) ä¸‰ä¸ªè¯„æµ‹é›†ä¸Šè¿›è¡Œäº†5-shotè¯„æµ‹ã€‚


### 7Bæ¨¡å‹ç»“æœ
| æ¨¡å‹åç§°                | C-Eval  | MMLU    | CMMLU   | AVERAGE |
|---------------------|---------|---------|---------|---------|
|                     |  5-shot |  5-shot |  5-shot |         |
| GPT-4*              | 68.7    | 86.4    | 71.0    | 75.4    |
| GPT-3.5*            | -       | 70.0    | -       | 70.0    |
| ChatGLM2-6B (base)* | 51.7    | 47.9    | -       | 4

### 13Bæ¨¡å‹ç»“æœ
| æ¨¡å‹åç§°                 | C-Eval 5-shot | MMLU 5-shot | CMMLU 5-shot | AVERAGE |
|-------------------------|---------------|-------------|--------------|---------|
|                         |  5-shot       |  5-shot     |  5-shot      |         |
| GPT-4*                  | 68.7          | 86.4        | 70.95        | 75.4    |
| GPT-3.5*                | -             | 70.0        | -            | 70.0    |
| ChatGLM-12B(base)*      | 61.6          | 56.2        | -            | 58.9    |



## æ³•å¾‹ã€åŒ»ç–—
æ³•å¾‹é¢†åŸŸæˆ‘ä»¬ä½¿ç”¨äº† [JEC-QA](https://jecqa.thunlp.org/) æ•°æ®é›†ã€‚æˆ‘ä»¬åªä¿ç•™äº†å…¶ä¸­çš„å•é€‰é¢˜ã€‚

åŒ»ç–—é¢†åŸŸåˆ™ä½¿ç”¨é€šç”¨é¢†åŸŸæ•°æ®é›†ï¼ˆCEval-valã€MMLUã€CMMLUï¼‰ä¸­çš„åŒ»å­¦ç›¸å…³å­¦ç§‘ã€[MedQA](https://huggingface.co/datasets/bigbio/med_qa)(USMLEã€MCMLE)å’Œ[MedMCQA](https://medmcqa.github.io/)ã€‚
- MedQAé‡‡ç”¨äº†äº”ä¸ªå€™é€‰çš„ç‰ˆæœ¬ï¼›
- MedMCQAåªä¿ç•™äº†å…¶ä¸­çš„å•é€‰é¢˜ï¼Œå¹¶ä¸”ä½¿ç”¨devé›†è¿›è¡Œæµ‹è¯•ï¼›
- é€šç”¨é¢†åŸŸæ•°æ®é›†åŒ…å«çš„åŒ»å­¦ç›¸å…³å­¦ç§‘å¦‚ä¸‹ï¼š
    - CEval: clinical_medicine,basic_medicine
    - MMLU: clinical_knowledge,anatomy,college_medicine,college_biology,nutrition,virology,medical_genetics,professional_medicine
    - CMMLU: anatomy,clinical_knowledge,college_medicine,genetics,nutrition,traditional_chinese_medicine,virology 

æˆ‘ä»¬é‡‡ç”¨å¯¹ä»¥ä¸Šæ•°æ®é›†è¿›è¡Œäº†5-shotæµ‹è¯•ã€‚
### 7Bæ¨¡å‹ç»“æœ

| æ¨¡å‹åç§°               | JEC-QA | CEval-MMLU-CMMLU | MedQA-USMLE | MedQA-MCMLE | MedExam | MedMCQA | AVERAGE |
|--------------------|--------|------------------|-------------|-------------|---------|---------|---------|
|                    | 5-shot |  5-shot          |  5-shot     |  5-shot     | 5-shot  | 5-shot  |         |
| GPT-4              | 59.32  | 77.16            | 80.28       | 74.58       | 75.10   | 72.51   | 75.93   |
| GPT-3.5            | 42.31  | 61.17            | 53.81       | 52.92       | 54.20   | 56.25   | 55.67   |
| LLaMA-7B           | 27.45  | 33.34            | 24.12       | 21.72       | 17.67   | 27.45   | 24.86   |


### 13Bæ¨¡å‹ç»“æœ
| æ¨¡å‹åç§°                    | æ³•å¾‹      | åŒ»ç–—        |
|-------------------------|---------|------------------|-------------|-------------|---------|---------|---------|
|                         | JEC-QA  | CEval-MMLU-CMMLU | MedQA-USMLE | MedQA-MCMLE | MedExam | MedMCQA | AVERAGE |
|                         |  5-shot |  5-shot          |  5-shot  53.33   | 42.08   | 46.46   |



## æ•°å­¦ã€ä»£ç 
æ•°å­¦é¢†åŸŸæˆ‘ä»¬å¯¹ [GSM8K](https://huggingface.co/datasets/gsm8k)å’Œ [MAtH](https://huggingface.co/datasets/competition_math)æ•°æ®é›†è¿›è¡Œäº† 4-shot æµ‹è¯•ã€‚

ä»£ç é¢†åŸŸåˆ™é‡‡ç”¨äº† [HumanEval](https://huggingface.co/datasets/openai_humaneval)å’Œ [MBPP](https://huggingface.co/datasets/mbpp) æ•°æ®é›†ï¼Œå¹¶å¯¹ HumanEval è¿›è¡Œäº† 0-shot æµ‹è¯•ï¼ŒMBPP æ•°æ®é›†è¿›è¡Œäº† 3-shot æµ‹è¯•ã€‚



### 7Bæ¨¡å‹ç»“æœ
| æ¨¡å‹åç§°              | GSM8K  | MATH   | AVERAGE | HumanEval | MBPP   | AVERAGE |
|---------------------|--------|--------|---------|-----------|--------|---------|
|                     | 4-shot | 4-shot |         |  0-shot   | 3-shot |         |
| ChatGLM2-6B (base)* | 32.37  |        | 32.4    |           |        |         |
| ChatGLM2-6B*        | 28.05  |        | 28.1    |           |        |         |
| InternLM-7B         | 31.2   | 7.1    | 19.2    | 10.4      |        |         |
| InternLM-7B-Chat    | 34.5   | 6.4    | 20.5    | 14        |        |         |
### 13Bæ¨¡å‹ç»“æœ

| æ•°å­¦                     |               |        | ä»£ç      |           |        |
|-------------------------|---------------|--------|---------|-----------|--------|
| æ¨¡å‹åç§°                 | GSM8K         | MATH   | AVERAGE | HumanEval | MBPP   | AVERAGE |
|                         | 4-shot        | 4-shot |         |  0-shot   | 3-shot |         |
| GPT-4*                  | 92 (5-shot)   | 42.5   |         | 67.0      |        |         |
| GPT-3.5*                | 57.1 (5-shot) |        |         | 48.1      |        |         |
| ChatGLM-12B(base)*      | 40.9          |        |         |           |        |         |



# æ¨ç†å’Œéƒ¨ç½²

æ¨ç†æ‰€éœ€çš„æ¨¡å‹æƒé‡ã€æºç ã€é…ç½®å·²å‘å¸ƒåœ¨ Hugging Faceï¼Œä¸‹è½½é“¾æ¥è§æœ¬æ–‡æ¡£æœ€å¼€å§‹çš„è¡¨æ ¼ã€‚ä¸‹é¢ä»¥ Baichuan2-13B-Chat ä¸ºä¾‹ç¤ºèŒƒå¤šç§æ¨ç†æ–¹å¼ã€‚ç¨‹åºä¼šè‡ªåŠ¨ä» Hugging Face ä¸‹è½½æ‰€éœ€èµ„æºã€‚

æ¨ç†å‰è¯·å®‰è£…ä¾èµ–ï¼š
```shell
pip install -r requirements.txt
```

## Pythonä»£ç æ–¹å¼

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> from transformers.generation.utils import GenerationConfig
>>> tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/Baichuan2-13B-Chat", use_fast=False, trust_remote_code=True)
>>> model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan2-13B-Chat", device_map="auto", torch_dtype=torch.float16, trust_remote_code=True)
>>> model.generation_config = GenerationConfig.from_pretrained("baichuan-inc/Baichuan2-13B-Chat")
>>> messages = []
>>> messages.append({"role": "user", "content": "ä¸–ç•Œä¸Šç¬¬äºŒé«˜çš„å±±å³°æ˜¯å“ªåº§"})
>>> response = model.chat(tokenizer, messages)
>>> print(response)
TODO æ”¹ä¸€ä¸ªæ–°çš„case
```

> åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œæ¨¡å‹åŠ è½½æŒ‡å®š `device_map='auto'`ï¼Œä¼šä½¿ç”¨æ‰€æœ‰å¯ç”¨æ˜¾å¡ã€‚å¦‚éœ€æŒ‡å®šä½¿ç”¨çš„è®¾å¤‡ï¼Œå¯ä»¥ä½¿ç”¨ç±»ä¼¼ `export CUDA_VISIBLE_DEVICES=0,1`ï¼ˆä½¿ç”¨äº†0ã€1å·æ˜¾å¡ï¼‰çš„æ–¹å¼æ§åˆ¶ã€‚


## å‘½ä»¤è¡Œå·¥å…·æ–¹å¼

```shell
python cli_demo.py
```
æœ€åè¾“å‡ºç¤ºä¾‹å¦‚ä¸‹ï¼š

<p align="center">
    <img src="media/cn-cli.png" width="70%"/>
</p>

## ç½‘é¡µ demo æ–¹å¼

ä¾é streamlitè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œä¼šåœ¨æœ¬åœ°å¯åŠ¨ä¸€ä¸ª web æœåŠ¡ï¼ŒæŠŠæ§åˆ¶å°ç»™å‡ºçš„åœ°å€æ”¾å…¥æµè§ˆå™¨å³å¯è®¿é—®ã€‚

```shell
streamlit run web_demo.py
```

æ•ˆæœå¦‚ä¸‹ï¼š

<p align="center">
    <img src="media/cn-web.gif" width="70%"/>
</p>

## Baichuan-13B-Chat ç¤ºä¾‹è¾“å‡º

<details><summary><b>å†…å®¹åˆ›ä½œ</b></summary>

```
ç”¨æˆ·ï¼š

```

</details>

<details><summary><b>å¹¿å‘Šæ–‡æ¡ˆ</b></summary>
  
```
ç”¨æˆ·ï¼š
```

</details>

<details><summary><b>ç²¾å‡†é—®ç­”</b></summary>

```
ç”¨æˆ·ï¼š
ä¸–ç•Œä¸Šç¬¬äºŒé«˜çš„å±±æ˜¯ä»€ä¹ˆå±±
```

</details>

<details><summary><b>è¯­è¨€ç†è§£</b></summary>

```
ç”¨æˆ·ï¼š
```
</details>


## æ¨ç†æ€§èƒ½
Baichuan-13B ä½¿ç”¨äº† ALiBi çº¿æ€§åç½®æŠ€æœ¯ï¼Œç›¸å¯¹äº Rotary Embedding è®¡ç®—é‡æ›´å°ï¼Œå¯¹æ¨ç†æ€§èƒ½æœ‰æ˜¾è‘—æå‡ï¼›ä¸æ ‡å‡†çš„ LLaMA-13B ç›¸æ¯”ï¼Œå¹³å‡æ¨ç†é€Ÿåº¦ (tokens/s) å®æµ‹æå‡ 31.6%ï¼š

| Model       | tokens/s |
|-------------|:--------:|
| LLaMA-13B   | ï¼Ÿï¼Ÿ     |
| Baichuan-13B| ï¼Ÿï¼Ÿ    |

> æµ‹è¯•ç¯å¢ƒå’Œå‚æ•°ï¼šGPU A100-SXM4-80G, PyTorch 2.0.0+cu117, transformers 4.29.1, batch size = 1, ç”Ÿæˆé•¿åº¦ = 2048, ç²¾åº¦ fp16, åŸºäº Baichuan-13B-Base


## é‡åŒ–éƒ¨ç½²

ä¸ºäº†è®©ä¸åŒçš„ç”¨æˆ·ä»¥åŠä¸åŒçš„å¹³å°éƒ½èƒ½è¿è¡ŒBaichuan2æ¨¡å‹ï¼Œæˆ‘ä»¬é’ˆå¯¹Baichuan2æ¨¡å‹åšäº†ç›¸åº”åœ°é‡åŒ–å·¥ä½œ(åŒ…æ‹¬Baichuan2-7B-Chatå’ŒBaichuan2-13B-Chat)ï¼Œæ–¹ä¾¿ç”¨æˆ·å¿«é€Ÿé«˜æ•ˆåœ°åœ¨è‡ªå·±çš„å¹³å°éƒ¨ç½²Baichuan2æ¨¡å‹ã€‚

### é‡åŒ–æ–¹æ³•

Baichuan2çš„é‡åŒ–æ–¹æ³•é‡‡ç”¨ç¤¾åŒºä¸»æµçš„é‡åŒ–æ–¹æ³•ï¼š[BitsAndBytesæ–¹æ³•](https://github.com/TimDettmers/bitsandbytes)ã€‚è¯¥æ–¹æ³•å¯ä»¥ä¿è¯é‡åŒ–åçš„æ•ˆæœåŸºæœ¬ä¸æ‰ç‚¹ï¼Œç›®å‰å·²ç»é›†æˆåˆ°transformersåº“é‡Œï¼Œå¹¶åœ¨ç¤¾åŒºå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚BitsAndBytesæ”¯æŒ4bitså’Œ8bitsä¸¤ç§é‡åŒ–ï¼Œå…¶ä¸­4bitsæ”¯æŒFP4å’ŒNF4ä¸¤ç§æ ¼å¼ï¼ŒBaichuan2é€‰ç”¨NF4ä½œä¸º4bité‡åŒ–çš„æ•°æ®ç±»å‹ã€‚  
  
åŸºäºè¯¥é‡åŒ–æ–¹æ³•ï¼ŒBaichuan2æ”¯æŒåœ¨çº¿é‡åŒ–å’Œç¦»çº¿é‡åŒ–ä¸¤ç§æ¨¡å¼ã€‚

### åœ¨çº¿é‡åŒ–

å¯¹äºåœ¨çº¿é‡åŒ–ï¼Œæˆ‘ä»¬æ”¯æŒ8bitså’Œ4bitsé‡åŒ–ï¼Œä½¿ç”¨æ–¹å¼å’Œ[Baichuan-13B](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat)æ–¹å¼ç±»ä¼¼ï¼Œåªéœ€è¦å…ˆåŠ è½½æ¨¡å‹åˆ°CPUçš„å†…å­˜é‡Œï¼Œå†è°ƒç”¨ä¸€ä¸ªquantizeæ¥å£é‡åŒ–ï¼Œæœ€åè°ƒç”¨cuda()å‡½æ•°ï¼Œå°†é‡åŒ–åçš„æƒé‡æ‹·è´åˆ°GPUæ˜¾å­˜ä¸­ã€‚å®ç°æ•´ä¸ªæ¨¡å‹åŠ è½½çš„ä»£ç éå¸¸ç®€å•ï¼Œæˆ‘ä»¬ä»¥Baichuan2-7B-Chatä¸ºä¾‹ï¼š  
8bitsåœ¨çº¿é‡åŒ–:
```python
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan2-7B-Chat", torch_dtype=torch.float16, trust_remote_code=True)
model = model.quantize(8).cuda() 
```
4bitsåœ¨çº¿é‡åŒ–:
```python
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan2-7B-Chat", torch_dtype=torch.float16, trust_remote_code=True)
model = model.quantize(4).cuda() 
```
éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ç”¨from_pretrainedæ¥å£çš„æ—¶å€™ï¼Œç”¨æˆ·ä¸€èˆ¬ä¼šåŠ ä¸Šdevice_map = "auto"ï¼Œåœ¨ä½¿ç”¨åœ¨çº¿é‡åŒ–æ—¶ï¼Œéœ€è¦å»æ‰è¿™ä¸ªå‚æ•°ï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚

### ç¦»çº¿é‡åŒ–
ä¸ºäº†æ–¹ä¾¿ç”¨æˆ·çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬æä¾›äº†ç¦»çº¿é‡åŒ–å¥½çš„4bitsçš„ç‰ˆæœ¬[Baichuan2-7B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat-4bits/tree/main)ï¼Œä¾›ç”¨æˆ·ä¸‹è½½ã€‚
ç”¨æˆ·åŠ è½½Baichuan2-7B-Chat-4bitsæ¨¡å‹å¾ˆç®€å•ï¼Œåªéœ€è¦æ‰§è¡Œ:
```python
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan2-7B-Chat-4bits", device_map="auto", trust_remote_code=True)
```
å¯¹äº8bitsç¦»çº¿é‡åŒ–ï¼Œæˆ‘ä»¬æ²¡æœ‰æä¾›ç›¸åº”çš„ç‰ˆæœ¬ï¼Œå› ä¸ºHuggingFace transformersåº“æä¾›äº†ç›¸åº”çš„APIæ¥å£ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿çš„å®ç°8bitsé‡åŒ–æ¨¡å‹çš„ä¿å­˜å’ŒåŠ è½½ã€‚ç”¨æˆ·å¯ä»¥è‡ªè¡ŒæŒ‰ç…§å¦‚ä¸‹æ–¹å¼å®ç°8bitsçš„æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ï¼š
```python
#æ¨¡å‹ä¿å­˜ï¼Œå…¶ä¸­model_idä¸ºåŸå§‹æ¨¡å‹ç›®å½•ï¼Œquant8_saved_dirä¸º8bitsé‡åŒ–åçš„æ¨¡å‹ä¿å­˜ç›®å½•
model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map="auto", trust_remote_code=True)
model.save_pretrained(quant8_saved_dir)

#æ¨¡å‹åŠ è½½
model = AutoModelForCausalLM.from_pretrained(quant8_saved_dir, device_map="auto", trust_remote_code=True)
```
### é‡åŒ–æ•ˆæœ
é‡åŒ–å‰åæ˜¾å­˜å ç”¨å¯¹æ¯”ï¼š
| Precision   | Baichuan2-7B GPU Mem (GB) |Baichuan2-13B GPU Mem (GB) |
|-------------|:------------:|:------------:|
| bf16 / fp16 | 14.0         | 25.9       |
| 8bits        | 8.0         | 14.2        |
| 4bits        | 5.1          | 8.6        |

é‡åŒ–ååœ¨å„ä¸ª benchmark ä¸Šçš„ç»“æœå’ŒåŸå§‹ç‰ˆæœ¬å¯¹æ¯”å¦‚ä¸‹ï¼š

| Model 5-shot           | C-Eval | MMLU | CMMLU |
|------------------------|:------:|:----:|:-----:|
| Baichuan2-13B-Chat      | 56.74  | 57.32| 59.68  |
| Baichuan2-13B-Chat-4bits | 56.05   | 56.24 | 58.82  |
| Baichuan2-7B-Chat       | 54.35   | 52.93 | 54.99  |
| Baichuan2-7B-Chat-4bits | 53.04   | 51.72 | 52.84  |

å¯ä»¥çœ‹åˆ°ï¼Œ4bitsç›¸å¯¹bfloat16æ‰ç‚¹åœ¨1~2ä¸ªç‚¹å·¦å³ã€‚

## CPUéƒ¨ç½²
Baichuan2æ¨¡å‹æ”¯æŒCPUæ¨ç†ï¼Œä½†éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼ŒCPUçš„æ¨ç†é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢ã€‚éœ€æŒ‰å¦‚ä¸‹æ–¹å¼ä¿®æ”¹æ¨¡å‹åŠ è½½çš„æ–¹å¼ï¼š
```python
#ä»¥Baichuan2-7B-Chatä¸ºä¾‹
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan2-7B-Chat", torch_dtype=torch.float32, trust_remote_code=True)
```
## Baichuan2ç›¸å¯¹Baichuan1æ¨ç†è¿ç§»
ç”±äºå¾ˆå¤šç”¨æˆ·åœ¨Baichuan1ä¸Šåšäº†å¾ˆå¤šä¼˜åŒ–çš„å·¥ä½œï¼Œä¾‹å¦‚ç¼–è¯‘ä¼˜åŒ–ã€é‡åŒ–ç­‰ï¼Œä¸ºäº†å°†è¿™äº›å·¥ä½œé›¶æˆæœ¬åœ°åº”ç”¨äºBaichuan2ï¼Œç”¨æˆ·å¯ä»¥å¯¹Baichuan2æ¨¡å‹åš1ä¸ªç¦»çº¿è½¬æ¢ï¼Œè½¬æ¢åå°±å¯ä»¥å½“åšBaichuan1æ¨¡å‹æ¥ä½¿ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œç”¨æˆ·åªéœ€è¦åˆ©ç”¨ä»¥ä¸‹è„šæœ¬ç¦»çº¿å¯¹Baichuan2æ¨¡å‹çš„æœ€åä¸€å±‚lm_headåšå½’ä¸€åŒ–ï¼Œå¹¶æ›¿æ¢æ‰â€lm_head.weightâ€œå³å¯ã€‚æ›¿æ¢å®Œåï¼Œå°±å¯ä»¥åƒå¯¹Baichuan1æ¨¡å‹ä¸€æ ·å¯¹è½¬æ¢åçš„æ¨¡å‹åšç¼–è¯‘ä¼˜åŒ–ç­‰å·¥ä½œäº†ã€‚
```python
import torch
import os
ori_model_dir = 'your baichuan2 model directory'
#ä¸ºäº†ä¸è¦†ç›–åŸå§‹æ¨¡å‹ï¼Œæœ€å¥½å°†è½¬æ¢åçš„æ¨¡å‹saveåˆ°å¦ä¸€ä¸ªç›®å½•å†æ›¿æ¢
new_model_dir = 'your normalized lm_head weight baichuan2 model directory'
model = torch.load(os.path.join(ori_model_dir, 'pytorch_model.bin'))
lm_head_w = model['lm_head.weight']
lm_head_w = torch.nn.functional.normalize(lm_head_w)
model['lm_head.weight'] = lm_head_w
torch.save(model, os.path.join(new_model_dir, 'pytorch_model.bin'))
```

# åº”ç”¨æ¡ˆä¾‹

# å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ

## ä¾èµ–å®‰è£…
```shell
git clone https://github.com/baichuan-inc/Baichuan2.git
cd Baichuan2/fine-tune
pip install -r requirements.txt
```
- å¦‚éœ€ä½¿ç”¨ LoRA ç­‰è½»é‡çº§å¾®è°ƒæ–¹æ³•éœ€é¢å¤–å®‰è£… [peft](https://github.com/huggingface/peft)
- å¦‚éœ€ä½¿ç”¨ xFormers è¿›è¡Œè®­ç»ƒåŠ é€Ÿéœ€é¢å¤–å®‰è£… [xFormers](https://github.com/facebookresearch/xformers)

## å•æœºè®­ç»ƒ

ä¸‹é¢æˆ‘ä»¬ç»™ä¸€ä¸ªå¾®è°ƒ Baichuan2-7B-Base çš„å•æœºè®­ç»ƒä¾‹å­ã€‚

è®­ç»ƒæ•°æ®ï¼š`data/belle_chat_ramdon_10k.json`ï¼Œè¯¥æ ·ä¾‹æ•°æ®æ˜¯ä» [multiturn_chat_0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M) é‡‡æ ·å‡º 1 ä¸‡æ¡ï¼Œå¹¶ä¸”åšäº†æ ¼å¼è½¬æ¢ã€‚ä¸»è¦æ˜¯å±•ç¤ºå¤šè½®æ•°æ®æ€ä¹ˆè®­ç»ƒï¼Œä¸ä¿è¯æ•ˆæœã€‚


```shell
hostfile=""
deepspeed --hostfile=$hostfile fine-tune.py  \
    --report_to "none" \
    --data_path "data/belle_chat_ramdon_10k.json" \
    --model_name_or_path "baichuan-inc/Baichuan2-7B-Base" \
    --output_dir "output" \
    --model_max_length 512 \
    --num_train_epochs 4 \
    --per_device_train_batch_size 16 \
    --gradient_accumulation_steps 1 \
    --save_strategy epoch \
    --learning_rate 2e-5 \
    --lr_scheduler_type constant \
    --adam_beta1 0.9 \
    --adam_beta2 0.98 \
    --adam_epsilon 1e-8 \
    --max_grad_norm 1.0 \
    --weight_decay 1e-4 \
    --warmup_ratio 0.0 \
    --logging_steps 1 \
    --gradient_checkpointing True \
    --deepspeed ds_config.json \
    --bf16 True \
    --tf32 True
```

## å¤šæœºè®­ç»ƒ

å¤šæœºè®­ç»ƒåªéœ€è¦ç»™ä¸€ä¸‹ hostfile ï¼Œå†…å®¹å¦‚ä¸‹ï¼š
```
ip1 slots=8
ip2 slots=8
ip3 slots=8
ip4 slots=8
```
åŒæ—¶åœ¨è®­ç»ƒè„šæœ¬é‡Œé¢æŒ‡å®š hosftfile çš„è·¯å¾„ï¼š
```shell
hostfile="/path/to/hostfile"
deepspeed --hostfile=$hostfile fine-tune.py  \
    --report_to "none" \
    --data_path "data/belle_chat_ramdon_10k.json" \
    --model_name_or_path "baichuan-inc/Baichuan2-7B-Base" \
    --output_dir "output" \
    --model_max_length 512 \
    --num_train_epochs 4 \
    --per_device_train_batch_size 16 \
    --gradient_accumulation_steps 1 \
    --save_strategy epoch \
    --learning_rate 2e-5 \
    --lr_scheduler_type constant \
    --adam_beta1 0.9 \
    --adam_beta2 0.98 \
    --adam_epsilon 1e-8 \
    --max_grad_norm 1.0 \
    --weight_decay 1e-4 \
    --warmup_ratio 0.0 \
    --logging_steps 1 \
    --gradient_checkpointing True \
    --deepspeed ds_config.json \
    --bf16 True \
    --tf32 True
```

## è½»é‡åŒ–å¾®è°ƒ

ä»£ç å·²ç»æ”¯æŒè½»é‡åŒ–å¾®è°ƒå¦‚ LoRAï¼Œå¦‚éœ€ä½¿ç”¨ä»…éœ€åœ¨ä¸Šé¢çš„è„šæœ¬ä¸­åŠ å…¥ä»¥ä¸‹å‚æ•°
```shell
--use_lora True
```
LoRA å…·ä½“çš„é…ç½®å¯è§ fine-tune.py è„šæœ¬ã€‚
ä½¿ç”¨ LoRA å¾®è°ƒåå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤åŠ è½½æ¨¡å‹
```python
from peft import AutoPeftModelForCausalLM
model = AutoPeftModelForCausalLM.from_pretrained("output", trust_remote_code=True)
```

# å†å² checkpoint
æˆ‘ä»¬æä¾›äº† Baichuan2-7B-Base æ¨¡å‹çš„ 10 ä¸ªå†å² checkpointï¼ˆ[ä¸‹è½½åœ°å€](http://hugging-face.co/baichuan-inc/Baichuan-7B-Base)ï¼‰ï¼Œä¾›ç ”ç©¶ä½¿ç”¨ã€‚

# ç¤¾åŒºæ”¯æŒ

# å£°æ˜

æˆ‘ä»¬åœ¨æ­¤å£°æ˜ï¼Œæˆ‘ä»¬çš„å¼€å‘å›¢é˜Ÿå¹¶æœªåŸºäº Baichuan2 æ¨¡å‹å¼€å‘ä»»ä½•åº”ç”¨ï¼Œæ— è®ºæ˜¯åœ¨ iOSã€Androidã€ç½‘é¡µæˆ–ä»»ä½•å…¶ä»–å¹³å°ã€‚æˆ‘ä»¬å¼ºçƒˆå‘¼åæ‰€æœ‰ä½¿ç”¨è€…ï¼Œä¸è¦åˆ©ç”¨ Baichuan2 æ¨¡å‹è¿›è¡Œä»»ä½•å±å®³å›½å®¶ç¤¾ä¼šå®‰å…¨æˆ–è¿æ³•çš„æ´»åŠ¨ã€‚å¦å¤–ï¼Œæˆ‘ä»¬ä¹Ÿè¦æ±‚ä½¿ç”¨è€…ä¸è¦å°† Baichuan2 æ¨¡å‹ç”¨äºæœªç»é€‚å½“å®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰çš„ä½¿ç”¨è€…éƒ½èƒ½éµå®ˆè¿™ä¸ªåŸåˆ™ï¼Œç¡®ä¿ç§‘æŠ€çš„å‘å±•èƒ½åœ¨è§„èŒƒå’Œåˆæ³•çš„ç¯å¢ƒä¸‹è¿›è¡Œã€‚

æˆ‘ä»¬å·²ç»å°½æˆ‘ä»¬æ‰€èƒ½ï¼Œæ¥ç¡®ä¿æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®çš„åˆè§„æ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡æˆ‘ä»¬å·²ç»åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å’Œæ•°æ®çš„å¤æ‚æ€§ï¼Œä»æœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ— æ³•é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚æœç”±äºä½¿ç”¨ Baichuan2 å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

# åè®®
å¯¹æœ¬ä»“åº“æºç çš„ä½¿ç”¨éµå¾ªå¼€æºè®¸å¯åè®® [Apache 2.0](https://github.com/baichuan-inc/Baichuan-13B/blob/main/LICENSE)ã€‚å¯¹ Baichuan2 æ¨¡å‹çš„ç¤¾åŒºä½¿ç”¨è§[ã€ŠBaichuan2 æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/Baichuan2%20%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)ã€‚Baichuan2 æ”¯æŒå•†ç”¨ã€‚å¦‚æœå°† Baichuan2 æ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨ä½œå•†ä¸šç”¨é€”ï¼Œè¯·æ‚¨æŒ‰ç…§å¦‚ä¸‹æ–¹å¼è”ç³»è®¸å¯æ–¹ï¼Œä»¥è¿›è¡Œç™»è®°å¹¶å‘è®¸å¯æ–¹ç”³è¯·ä¹¦é¢æˆæƒï¼šè”ç³»é‚®ç®± <opensource@baichuan-inc.com>ã€‚
