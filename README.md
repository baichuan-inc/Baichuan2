<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->

<div align="center">
<h1>
  Baichuan2
</h1>
</div>

<p align="center">
ğŸ¤— <a href="https://huggingface.co/baichuan-inc/" target="_blank">Hugging Face</a> â€¢ ğŸ¤– <a href="https://modelscope.cn/organization/baichuan-inc" target="_blank">ModelScope</a> â€¢ ğŸ’¬ <a href="https://github.com/baichuan-inc/Baichuan2/blob/main/media/wechat.jpeg?raw=true" target="_blank">WeChat</a>
</p>

<div align="center">

[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/baichuan-inc/Baichuan2/blob/main/LICENSE)
<h4 align="center">
    <p>
        <b>ä¸­æ–‡</b> |
        <a href="https://github.com/baichuan-inc/Baichuan2/blob/main/README_EN.md">English</a>
    <p>
</h4>
</div>

# ç›®å½•

- [ä»‹ç»](#ä»‹ç»)
- [Benchmarkç»“æœ](#Benchmark-ç»“æœ)
- [æ¨¡å‹ç»†èŠ‚](#æ¨¡å‹ç»†èŠ‚)
- [æ¨ç†å’Œéƒ¨ç½²](#æ¨ç†å’Œéƒ¨ç½²)
- [åº”ç”¨æ¡ˆä¾‹](#åº”ç”¨æ¡ˆä¾‹)
- [å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ](#å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ)
- [å†å² checkpoint](#å†å²-checkpoint)
- [ç¤¾åŒºæ”¯æŒ](#ç¤¾åŒºæ”¯æŒ)
- [å£°æ˜](#å£°æ˜)
- [åè®®](#åè®®)

# ä»‹ç»

Baichuan2 æ˜¯ç”±ç™¾å·æ™ºèƒ½ç»§ Baichuan-7B å’Œ Baichuan-13B ä¹‹ååšå‡ºçš„æ›´æ–°ç‰ˆæœ¬ï¼Œé‡‡ç”¨ 2.6 ä¸‡äº¿  Tokens çš„é«˜è´¨é‡è¯­æ–™è®­ç»ƒï¼Œåœ¨æƒå¨çš„ä¸­æ–‡å’Œè‹±æ–‡ benchmark ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚æœ¬æ¬¡å‘å¸ƒåŒ…å«æœ‰ 7Bã€13B çš„ Base å’Œ Chat ç‰ˆæœ¬ï¼Œå¹¶æä¾›äº† Chat ç‰ˆæœ¬çš„ 4bits é‡åŒ–ï¼Œæ‰€æœ‰ç‰ˆæœ¬ä¸ä»…å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¼€å‘è€…ä¹Ÿä»…éœ€é‚®ä»¶ç”³è¯·å¹¶è·å¾—å®˜æ–¹å•†ç”¨è®¸å¯åï¼Œå³å¯ä»¥å…è´¹å•†ç”¨ã€‚å…·ä½“å‘å¸ƒç‰ˆæœ¬å’Œä¸‹è½½è§ä¸‹è¡¨ï¼š
|         | åŸºåº§æ¨¡å‹  | å¯¹é½æ¨¡å‹ | å¯¹é½æ¨¡å‹ 4bits é‡åŒ– |
|:-------:|:-------:|:-------:|:-----------------:|
| 7B      | [Baichuan2-7B-Base](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base) |[Baichuan2-7B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat) |[Baichuan2-7B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat-4bits) |
| 13B     | [Baichuan2-13B-Base](https://huggingface.co/baichuan-inc/Baichuan2-13B-Base) |[Baichuan2-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat) |[Baichuan2-13B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat-4bits) |


# Benchmark ç»“æœ
updating @ chengwei



# æ¨ç†å’Œéƒ¨ç½²

æ¨ç†æ‰€éœ€çš„æ¨¡å‹æƒé‡ã€æºç ã€é…ç½®å·²å‘å¸ƒåœ¨ Hugging Faceï¼Œä¸‹è½½é“¾æ¥è§æœ¬æ–‡æ¡£æœ€å¼€å§‹çš„è¡¨æ ¼ã€‚ä¸‹é¢ä»¥ Baichuan2-13B-Chat ä¸ºä¾‹ç¤ºèŒƒå¤šç§æ¨ç†æ–¹å¼ã€‚ç¨‹åºä¼šè‡ªåŠ¨ä» Hugging Face ä¸‹è½½æ‰€éœ€èµ„æºã€‚

æ¨ç†å‰è¯·å®‰è£…ä¾èµ–ï¼š
```shell
pip install -r requirements.txt
```

## Pythonä»£ç æ–¹å¼

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> from transformers.generation.utils import GenerationConfig
>>> tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/Baichuan2-13B-Chat", use_fast=False, trust_remote_code=True)
>>> model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan2-13B-Chat", device_map="auto", torch_dtype=torch.float16, trust_remote_code=True)
>>> model.generation_config = GenerationConfig.from_pretrained("baichuan-inc/Baichuan2-13B-Chat")
>>> messages = []
>>> messages.append({"role": "user", "content": "ä¸–ç•Œä¸Šç¬¬äºŒé«˜çš„å±±å³°æ˜¯å“ªåº§"})
>>> response = model.chat(tokenizer, messages)
>>> print(response)
TODO æ”¹ä¸€ä¸ªæ–°çš„case
```

> åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œæ¨¡å‹åŠ è½½æŒ‡å®š `device_map='auto'`ï¼Œä¼šä½¿ç”¨æ‰€æœ‰å¯ç”¨æ˜¾å¡ã€‚å¦‚éœ€æŒ‡å®šä½¿ç”¨çš„è®¾å¤‡ï¼Œå¯ä»¥ä½¿ç”¨ç±»ä¼¼ `export CUDA_VISIBLE_DEVICES=0,1`ï¼ˆä½¿ç”¨äº†0ã€1å·æ˜¾å¡ï¼‰çš„æ–¹å¼æ§åˆ¶ã€‚


## å‘½ä»¤è¡Œå·¥å…·æ–¹å¼

```shell
python cli_demo.py
```
æœ€åè¾“å‡ºç¤ºä¾‹å¦‚ä¸‹ï¼š

<p align="center">
    <img src="media/cn-cli.png" width="70%"/>
</p>

## ç½‘é¡µ demo æ–¹å¼

ä¾é streamlitè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œä¼šåœ¨æœ¬åœ°å¯åŠ¨ä¸€ä¸ª web æœåŠ¡ï¼ŒæŠŠæ§åˆ¶å°ç»™å‡ºçš„åœ°å€æ”¾å…¥æµè§ˆå™¨å³å¯è®¿é—®ã€‚

```shell
streamlit run web_demo.py
```

æ•ˆæœå¦‚ä¸‹ï¼š

<p align="center">
    <img src="media/cn-web.gif" width="70%"/>
</p>

## Baichuan-13B-Chat ç¤ºä¾‹è¾“å‡º

<details><summary><b>å†…å®¹åˆ›ä½œ</b></summary>

```
ç”¨æˆ·ï¼š

```

</details>

<details><summary><b>å¹¿å‘Šæ–‡æ¡ˆ</b></summary>
  
```
ç”¨æˆ·ï¼š
```

</details>

<details><summary><b>ç²¾å‡†é—®ç­”</b></summary>

```
ç”¨æˆ·ï¼š
ä¸–ç•Œä¸Šç¬¬äºŒé«˜çš„å±±æ˜¯ä»€ä¹ˆå±±
```

</details>

<details><summary><b>è¯­è¨€ç†è§£</b></summary>

```
ç”¨æˆ·ï¼š
```
</details>


## æ¨ç†æ€§èƒ½
Baichuan-13B ä½¿ç”¨äº† ALiBi çº¿æ€§åç½®æŠ€æœ¯ï¼Œç›¸å¯¹äº Rotary Embedding è®¡ç®—é‡æ›´å°ï¼Œå¯¹æ¨ç†æ€§èƒ½æœ‰æ˜¾è‘—æå‡ï¼›ä¸æ ‡å‡†çš„ LLaMA-13B ç›¸æ¯”ï¼Œå¹³å‡æ¨ç†é€Ÿåº¦ (tokens/s) å®æµ‹æå‡ 31.6%ï¼š

| Model       | tokens/s |
|-------------|:--------:|
| LLaMA-13B   | ï¼Ÿï¼Ÿ     |
| Baichuan-13B| ï¼Ÿï¼Ÿ    |

> æµ‹è¯•ç¯å¢ƒå’Œå‚æ•°ï¼šGPU A100-SXM4-80G, PyTorch 2.0.0+cu117, transformers 4.29.1, batch size = 1, ç”Ÿæˆé•¿åº¦ = 2048, ç²¾åº¦ fp16, åŸºäº Baichuan-13B-Base


## é‡åŒ–éƒ¨ç½²

ä¸ºäº†è®©ä¸åŒçš„ç”¨æˆ·ä»¥åŠä¸åŒçš„å¹³å°éƒ½èƒ½è¿è¡ŒBaichuan2æ¨¡å‹ï¼Œæˆ‘ä»¬é’ˆå¯¹Baichuan2æ¨¡å‹åšäº†ç›¸åº”åœ°é‡åŒ–å·¥ä½œ(åŒ…æ‹¬Baichuan2-7B-Chatå’ŒBaichuan2-13B-Chat)ï¼Œæ–¹ä¾¿ç”¨æˆ·å¿«é€Ÿé«˜æ•ˆåœ°åœ¨è‡ªå·±çš„å¹³å°éƒ¨ç½²Baichuan2æ¨¡å‹ã€‚

### é‡åŒ–æ–¹æ³•

Baichuan2çš„é‡åŒ–æ–¹æ³•é‡‡ç”¨ç¤¾åŒºä¸»æµçš„é‡åŒ–æ–¹æ³•ï¼š[BitsAndBytesæ–¹æ³•](https://github.com/TimDettmers/bitsandbytes)ã€‚è¯¥æ–¹æ³•å¯ä»¥ä¿è¯é‡åŒ–åçš„æ•ˆæœåŸºæœ¬ä¸æ‰ç‚¹ï¼Œç›®å‰å·²ç»é›†æˆåˆ°transformersåº“é‡Œï¼Œå¹¶åœ¨ç¤¾åŒºå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚BitsAndBytesæ”¯æŒ4bitså’Œ8bitsä¸¤ç§é‡åŒ–ï¼Œå…¶ä¸­4bitsæ”¯æŒFP4å’ŒNF4ä¸¤ç§æ ¼å¼ï¼ŒBaichuan2é€‰ç”¨NF4ä½œä¸º4bité‡åŒ–çš„æ•°æ®ç±»å‹ã€‚  
  
åŸºäºè¯¥é‡åŒ–æ–¹æ³•ï¼ŒBaichuan2æ”¯æŒåœ¨çº¿é‡åŒ–å’Œç¦»çº¿é‡åŒ–ä¸¤ç§æ¨¡å¼ã€‚

### åœ¨çº¿é‡åŒ–

å¯¹äºåœ¨çº¿é‡åŒ–ï¼Œæˆ‘ä»¬æ”¯æŒ8bitså’Œ4bitsé‡åŒ–ï¼Œä½¿ç”¨æ–¹å¼å’Œ[Baichuan-13B](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat)æ–¹å¼ç±»ä¼¼ï¼Œåªéœ€è¦å…ˆåŠ è½½æ¨¡å‹åˆ°CPUçš„å†…å­˜é‡Œï¼Œå†è°ƒç”¨ä¸€ä¸ªquantizeæ¥å£é‡åŒ–ï¼Œæœ€åè°ƒç”¨cuda()å‡½æ•°ï¼Œå°†é‡åŒ–åçš„æƒé‡æ‹·è´åˆ°GPUæ˜¾å­˜ä¸­ã€‚å®ç°æ•´ä¸ªæ¨¡å‹åŠ è½½çš„ä»£ç éå¸¸ç®€å•ï¼Œæˆ‘ä»¬ä»¥Baichuan2-7B-Chatä¸ºä¾‹ï¼š  
8bitsåœ¨çº¿é‡åŒ–:
```python
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan2-7B-Chat", torch_dtype=torch.float16, trust_remote_code=True)
model = model.quantize(8).cuda() 
```
4bitsåœ¨çº¿é‡åŒ–:
```python
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan2-7B-Chat", torch_dtype=torch.float16, trust_remote_code=True)
model = model.quantize(4).cuda() 
```
éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ç”¨from_pretrainedæ¥å£çš„æ—¶å€™ï¼Œç”¨æˆ·ä¸€èˆ¬ä¼šåŠ ä¸Šdevice_map = "auto"ï¼Œåœ¨ä½¿ç”¨åœ¨çº¿é‡åŒ–æ—¶ï¼Œéœ€è¦å»æ‰è¿™ä¸ªå‚æ•°ï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚

### ç¦»çº¿é‡åŒ–
ä¸ºäº†æ–¹ä¾¿ç”¨æˆ·çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬æä¾›äº†ç¦»çº¿é‡åŒ–å¥½çš„4bitsçš„ç‰ˆæœ¬[Baichuan2-7B-Chat-4bits](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat-4bits/tree/main)ï¼Œä¾›ç”¨æˆ·ä¸‹è½½ã€‚
ç”¨æˆ·åŠ è½½Baichuan2-7B-Chat-4bitsæ¨¡å‹å¾ˆç®€å•ï¼Œåªéœ€è¦æ‰§è¡Œ:
```python
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan2-7B-Chat-4bits", device_map="auto", trust_remote_code=True)
```
å¯¹äº8bitsç¦»çº¿é‡åŒ–ï¼Œæˆ‘ä»¬æ²¡æœ‰æä¾›ç›¸åº”çš„ç‰ˆæœ¬ï¼Œå› ä¸ºHuggingFace transformersåº“æä¾›äº†ç›¸åº”çš„APIæ¥å£ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿çš„å®ç°8bitsé‡åŒ–æ¨¡å‹çš„ä¿å­˜å’ŒåŠ è½½ã€‚ç”¨æˆ·å¯ä»¥è‡ªè¡ŒæŒ‰ç…§å¦‚ä¸‹æ–¹å¼å®ç°8bitsçš„æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ï¼š
```python
#æ¨¡å‹ä¿å­˜ï¼Œå…¶ä¸­model_idä¸ºåŸå§‹æ¨¡å‹ç›®å½•ï¼Œquant8_saved_dirä¸º8bitsé‡åŒ–åçš„æ¨¡å‹ä¿å­˜ç›®å½•
model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map="auto", trust_remote_code=True)
model.save_pretrained(quant8_saved_dir)

#æ¨¡å‹åŠ è½½
model = AutoModelForCausalLM.from_pretrained(quant8_saved_dir, device_map="auto", trust_remote_code=True)
```
### é‡åŒ–æ•ˆæœ
é‡åŒ–å‰åæ˜¾å­˜å ç”¨å¯¹æ¯”ï¼š
| Precision   | Baichuan2-7B GPU Mem (GB) |Baichuan2-13B GPU Mem (GB) |
|-------------|:------------:|:------------:|
| bf16 / fp16 | 14.0         | 25.9       |
| 8bits        | 8.0         | 14.2        |
| 4bits        | 5.1          | 8.6        |

é‡åŒ–ååœ¨å„ä¸ª benchmark ä¸Šçš„ç»“æœå’ŒåŸå§‹ç‰ˆæœ¬å¯¹æ¯”å¦‚ä¸‹ï¼š

| Model 5-shot           | C-Eval | MMLU | CMMLU |
|------------------------|:------:|:----:|:-----:|
| Baichuan2-13B-Chat      | 56.74  | 57.32| 59.68  |
| Baichuan2-13B-Chat-4bits | 56.05   | 56.24 | 58.82  |
| Baichuan2-7B-Chat       | 54.35   | 52.93 | 54.99  |
| Baichuan2-7B-Chat-4bits | 53.04   | 51.72 | 52.84  |

å¯ä»¥çœ‹åˆ°ï¼Œ4bitsç›¸å¯¹bfloat16æ‰ç‚¹åœ¨1~2ä¸ªç‚¹å·¦å³ã€‚

## CPUéƒ¨ç½²
Baichuan2æ¨¡å‹æ”¯æŒCPUæ¨ç†ï¼Œä½†éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼ŒCPUçš„æ¨ç†é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢ã€‚éœ€æŒ‰å¦‚ä¸‹æ–¹å¼ä¿®æ”¹æ¨¡å‹åŠ è½½çš„æ–¹å¼ï¼š
```python
#ä»¥Baichuan2-7B-Chatä¸ºä¾‹
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan2-7B-Chat", torch_dtype=torch.float32, trust_remote_code=True)
```
## Baichuan2 ç›¸å¯¹ Baichuan æ¨ç†è¿ç§»
ç”±äºå¾ˆå¤šç”¨æˆ·åœ¨ Baichuan(Baichuan-7B, Baichuan-13B)ä¸Šåšäº†å¾ˆå¤šä¼˜åŒ–çš„å·¥ä½œï¼Œä¾‹å¦‚ç¼–è¯‘ä¼˜åŒ–ã€é‡åŒ–ç­‰ï¼Œä¸ºäº†å°†è¿™äº›å·¥ä½œé›¶æˆæœ¬åœ°åº”ç”¨äº Baichuan2ï¼Œç”¨æˆ·å¯ä»¥å¯¹ Baichuan2 æ¨¡å‹åšä¸€ä¸ªç¦»çº¿è½¬æ¢ï¼Œè½¬æ¢åå°±å¯ä»¥å½“åš Baichuan æ¨¡å‹æ¥ä½¿ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œç”¨æˆ·åªéœ€è¦åˆ©ç”¨ä»¥ä¸‹è„šæœ¬ç¦»çº¿å¯¹ Baichuan2 æ¨¡å‹çš„æœ€åä¸€å±‚lm_headåšå½’ä¸€åŒ–ï¼Œå¹¶æ›¿æ¢æ‰â€lm_head.weightâ€œå³å¯ã€‚æ›¿æ¢å®Œåï¼Œå°±å¯ä»¥åƒå¯¹ Baichuan æ¨¡å‹ä¸€æ ·å¯¹è½¬æ¢åçš„æ¨¡å‹åšç¼–è¯‘ä¼˜åŒ–ç­‰å·¥ä½œäº†ã€‚
```python
import torch
import os
ori_model_dir = 'your baichuan2 model directory'
# ä¸ºäº†ä¸è¦†ç›–åŸå§‹æ¨¡å‹ï¼Œæœ€å¥½å°†è½¬æ¢åçš„æ¨¡å‹saveåˆ°å¦ä¸€ä¸ªç›®å½•å†æ›¿æ¢
new_model_dir = 'your normalized lm_head weight baichuan2 model directory'
model = torch.load(os.path.join(ori_model_dir, 'pytorch_model.bin'))
lm_head_w = model['lm_head.weight']
lm_head_w = torch.nn.functional.normalize(lm_head_w)
model['lm_head.weight'] = lm_head_w
torch.save(model, os.path.join(new_model_dir, 'pytorch_model.bin'))
```

# åº”ç”¨æ¡ˆä¾‹

# å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ

## ä¾èµ–å®‰è£…
```shell
git clone https://github.com/baichuan-inc/Baichuan2.git
cd Baichuan2/fine-tune
pip install -r requirements.txt
```
- å¦‚éœ€ä½¿ç”¨ LoRA ç­‰è½»é‡çº§å¾®è°ƒæ–¹æ³•éœ€é¢å¤–å®‰è£… [peft](https://github.com/huggingface/peft)
- å¦‚éœ€ä½¿ç”¨ xFormers è¿›è¡Œè®­ç»ƒåŠ é€Ÿéœ€é¢å¤–å®‰è£… [xFormers](https://github.com/facebookresearch/xformers)

## å•æœºè®­ç»ƒ

ä¸‹é¢æˆ‘ä»¬ç»™ä¸€ä¸ªå¾®è°ƒ Baichuan2-7B-Base çš„å•æœºè®­ç»ƒä¾‹å­ã€‚

è®­ç»ƒæ•°æ®ï¼š`data/belle_chat_ramdon_10k.json`ï¼Œè¯¥æ ·ä¾‹æ•°æ®æ˜¯ä» [multiturn_chat_0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M) é‡‡æ ·å‡º 1 ä¸‡æ¡ï¼Œå¹¶ä¸”åšäº†æ ¼å¼è½¬æ¢ã€‚ä¸»è¦æ˜¯å±•ç¤ºå¤šè½®æ•°æ®æ€ä¹ˆè®­ç»ƒï¼Œä¸ä¿è¯æ•ˆæœã€‚


```shell
hostfile=""
deepspeed --hostfile=$hostfile fine-tune.py  \
    --report_to "none" \
    --data_path "data/belle_chat_ramdon_10k.json" \
    --model_name_or_path "baichuan-inc/Baichuan2-7B-Base" \
    --output_dir "output" \
    --model_max_length 512 \
    --num_train_epochs 4 \
    --per_device_train_batch_size 16 \
    --gradient_accumulation_steps 1 \
    --save_strategy epoch \
    --learning_rate 2e-5 \
    --lr_scheduler_type constant \
    --adam_beta1 0.9 \
    --adam_beta2 0.98 \
    --adam_epsilon 1e-8 \
    --max_grad_norm 1.0 \
    --weight_decay 1e-4 \
    --warmup_ratio 0.0 \
    --logging_steps 1 \
    --gradient_checkpointing True \
    --deepspeed ds_config.json \
    --bf16 True \
    --tf32 True
```

## å¤šæœºè®­ç»ƒ

å¤šæœºè®­ç»ƒåªéœ€è¦ç»™ä¸€ä¸‹ hostfile ï¼Œå†…å®¹å¦‚ä¸‹ï¼š
```
ip1 slots=8
ip2 slots=8
ip3 slots=8
ip4 slots=8
```
åŒæ—¶åœ¨è®­ç»ƒè„šæœ¬é‡Œé¢æŒ‡å®š hosftfile çš„è·¯å¾„ï¼š
```shell
hostfile="/path/to/hostfile"
deepspeed --hostfile=$hostfile fine-tune.py  \
    --report_to "none" \
    --data_path "data/belle_chat_ramdon_10k.json" \
    --model_name_or_path "baichuan-inc/Baichuan2-7B-Base" \
    --output_dir "output" \
    --model_max_length 512 \
    --num_train_epochs 4 \
    --per_device_train_batch_size 16 \
    --gradient_accumulation_steps 1 \
    --save_strategy epoch \
    --learning_rate 2e-5 \
    --lr_scheduler_type constant \
    --adam_beta1 0.9 \
    --adam_beta2 0.98 \
    --adam_epsilon 1e-8 \
    --max_grad_norm 1.0 \
    --weight_decay 1e-4 \
    --warmup_ratio 0.0 \
    --logging_steps 1 \
    --gradient_checkpointing True \
    --deepspeed ds_config.json \
    --bf16 True \
    --tf32 True
```

## è½»é‡åŒ–å¾®è°ƒ

ä»£ç å·²ç»æ”¯æŒè½»é‡åŒ–å¾®è°ƒå¦‚ LoRAï¼Œå¦‚éœ€ä½¿ç”¨ä»…éœ€åœ¨ä¸Šé¢çš„è„šæœ¬ä¸­åŠ å…¥ä»¥ä¸‹å‚æ•°
```shell
--use_lora True
```
LoRA å…·ä½“çš„é…ç½®å¯è§ fine-tune.py è„šæœ¬ã€‚
ä½¿ç”¨ LoRA å¾®è°ƒåå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤åŠ è½½æ¨¡å‹
```python
from peft import AutoPeftModelForCausalLM
model = AutoPeftModelForCausalLM.from_pretrained("output", trust_remote_code=True)
```

# å†å² checkpoint
æˆ‘ä»¬æä¾›äº† Baichuan2-7B-Base æ¨¡å‹çš„ 10 ä¸ªå†å² checkpointï¼ˆ[ä¸‹è½½åœ°å€](http://hugging-face.co/baichuan-inc/Baichuan-7B-Base)ï¼‰ï¼Œä¾›ç ”ç©¶ä½¿ç”¨ã€‚

# ç¤¾åŒºæ”¯æŒ

## åä¸ºæ˜‡è…¾
**æ˜‡è…¾NPUå¾®è°ƒ**ï¼šBaichuan2 æ”¯æŒåŸºäºæ˜‡è…¾ NPU çš„ PyTorch + DeepSpeed æ¨¡å‹å¾®è°ƒï¼Œå¾®è°ƒæ‰€éœ€çš„ modelingã€READMEã€ç¤ºä¾‹è„šæœ¬å·²å‘å¸ƒï¼š[7B](https://gitee.com/ascend/ModelZoo-PyTorch/tree/master/PyTorch/built-in/foundation/Baichuan2/7B)ã€‚13B æ”¯æŒæ­£åœ¨å¼€å‘ä¸­ã€‚

**æ˜‡è…¾NPUéƒ¨ç½²**ï¼šBaichuan2 æ”¯æŒæ˜‡è…¾ NPU æ¨ç†ï¼Œæ¨ç†æ‰€éœ€çš„ modelingã€READMEã€ç¤ºä¾‹è„šæœ¬å·²å‘å¸ƒï¼š[7B](https://gitee.com/ascend/ModelZoo-PyTorch/tree/master/ACL_PyTorch/built-in/foundation_models/baichuan2/7b)ã€[13B](https://gitee.com/ascend/ModelZoo-PyTorch/tree/master/ACL_PyTorch/built-in/foundation_models/baichuan2/13b)ã€‚

**MindFormerså¥—ä»¶**ï¼š[MindFormers](https://gitee.com/mindspore/mindformers) æ˜¯æ„å»ºä¸€ä¸ªåŸºäºæ˜‡æ€æ¡†æ¶(MindSpore)å¤§æ¨¡å‹è®­ç»ƒã€å¾®è°ƒã€è¯„ä¼°ã€æ¨ç†ã€éƒ¨ç½²çš„å…¨æµç¨‹å¼€å‘å¥—ä»¶ï¼Œå½“å‰é›†æˆäº†[Baichuan2](https://gitee.com/mindspore/mindformers/tree/dev/research/baichuan2)ï¼Œæ”¯æŒç”¨æˆ·è¿›è¡Œæ¨¡å‹éƒ¨ç½²ã€å¾®è°ƒè®­ç»ƒå’Œåˆ›æ–°ç ”å‘ã€‚

**å¤§æ¨¡å‹ä½“éªŒå¹³å°**ï¼š[æ˜‡æ€å¤§æ¨¡å‹å¹³å°](https://xihe.mindspore.cn) åŸºäºæ˜‡æ€ MindSpore AI æ¡†æ¶ã€MindFormers å¤§æ¨¡å‹å¼€å‘å¥—ä»¶ä¸æ˜‡è…¾ç¡¬ä»¶ç®—åŠ›ï¼Œå°† [Baichuan2-7B](https://xihe.mindspore.cn/modelzoo/baichuan) å¤§æ¨¡å‹èƒ½åŠ›å¼€æ”¾ç»™å…¬ä¼—ï¼Œæ¬¢è¿å¤§å®¶ä½¿ç”¨ã€‚


# å£°æ˜

æˆ‘ä»¬åœ¨æ­¤å£°æ˜ï¼Œæˆ‘ä»¬çš„å¼€å‘å›¢é˜Ÿå¹¶æœªåŸºäº Baichuan2 æ¨¡å‹å¼€å‘ä»»ä½•åº”ç”¨ï¼Œæ— è®ºæ˜¯åœ¨ iOSã€Androidã€ç½‘é¡µæˆ–ä»»ä½•å…¶ä»–å¹³å°ã€‚æˆ‘ä»¬å¼ºçƒˆå‘¼åæ‰€æœ‰ä½¿ç”¨è€…ï¼Œä¸è¦åˆ©ç”¨ Baichuan2 æ¨¡å‹è¿›è¡Œä»»ä½•å±å®³å›½å®¶ç¤¾ä¼šå®‰å…¨æˆ–è¿æ³•çš„æ´»åŠ¨ã€‚å¦å¤–ï¼Œæˆ‘ä»¬ä¹Ÿè¦æ±‚ä½¿ç”¨è€…ä¸è¦å°† Baichuan2 æ¨¡å‹ç”¨äºæœªç»é€‚å½“å®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰çš„ä½¿ç”¨è€…éƒ½èƒ½éµå®ˆè¿™ä¸ªåŸåˆ™ï¼Œç¡®ä¿ç§‘æŠ€çš„å‘å±•èƒ½åœ¨è§„èŒƒå’Œåˆæ³•çš„ç¯å¢ƒä¸‹è¿›è¡Œã€‚

æˆ‘ä»¬å·²ç»å°½æˆ‘ä»¬æ‰€èƒ½ï¼Œæ¥ç¡®ä¿æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®çš„åˆè§„æ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡æˆ‘ä»¬å·²ç»åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å’Œæ•°æ®çš„å¤æ‚æ€§ï¼Œä»æœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ— æ³•é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚æœç”±äºä½¿ç”¨ Baichuan2 å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

# åè®®
å¯¹æœ¬ä»“åº“æºç çš„ä½¿ç”¨éµå¾ªå¼€æºè®¸å¯åè®® [Apache 2.0](https://github.com/baichuan-inc/Baichuan-13B/blob/main/LICENSE)ã€‚å¯¹ Baichuan2 æ¨¡å‹çš„ç¤¾åŒºä½¿ç”¨è§[ã€ŠBaichuan2 æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/Baichuan2%20%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)ã€‚Baichuan2 æ”¯æŒå•†ç”¨ã€‚å¦‚æœå°† Baichuan2 æ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨ä½œå•†ä¸šç”¨é€”ï¼Œè¯·æ‚¨æŒ‰ç…§å¦‚ä¸‹æ–¹å¼è”ç³»è®¸å¯æ–¹ï¼Œä»¥è¿›è¡Œç™»è®°å¹¶å‘è®¸å¯æ–¹ç”³è¯·ä¹¦é¢æˆæƒï¼šè”ç³»é‚®ç®± <opensource@baichuan-inc.com>ã€‚
